{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mse99an\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Python\\repos_python\\Project_Timeseries_Generation\\wandb\\run-20240324_211258-6pqto32j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/se99an/Time-GAN/runs/6pqto32j' target=\"_blank\">winter-shadow-18</a></strong> to <a href='https://wandb.ai/se99an/Time-GAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/se99an/Time-GAN' target=\"_blank\">https://wandb.ai/se99an/Time-GAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/se99an/Time-GAN/runs/6pqto32j' target=\"_blank\">https://wandb.ai/se99an/Time-GAN/runs/6pqto32j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "project_name = \"Time-GAN\"\n",
    "\n",
    "wandb.init(project=project_name)\n",
    "wandb.run.name = \"TEST\"\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, _ = self.rnn(x)\n",
    "        output = torch.sigmoid(self.fc(r_out))\n",
    "        return output\n",
    "\n",
    "\n",
    "class Recovery(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super(Recovery, self).__init__()\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        r_out, _ = self.rnn(h)\n",
    "        output = torch.sigmoid(self.fc(r_out))\n",
    "        return output\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        r_out, _ = self.rnn(z)\n",
    "        output = torch.sigmoid(self.fc(r_out))\n",
    "        return output\n",
    "\n",
    "\n",
    "class Supervisor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(Supervisor, self).__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers-1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        r_out, _ = self.rnn(h)\n",
    "        output = torch.sigmoid(self.fc(r_out))\n",
    "        return output\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        r_out, _ = self.rnn(h)\n",
    "        output = torch.sigmoid(self.fc(r_out))\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb  # Ensure you've initialized wandb and logged in appropriately.\n",
    "\n",
    "def train_GAN(num_epochs_embedding, num_epochs_supervised, num_epochs_adversarial, data_loader, batch_size, seq_len, hidden_dim, embedder, recovery, generator, supervisor, discriminator, E_optimizer, G_optimizer, D_optimizer):\n",
    "    # Embedding Phase: Training Embedder and Recovery\n",
    "    for epoch in range(num_epochs_embedding):\n",
    "        for X_mb, T_mb in data_loader:  # Iterate through mini-batches from the data loader.\n",
    "            H_mb = embedder(X_mb)\n",
    "            X_tilde = recovery(H_mb)\n",
    "            E_loss = torch.mean((X_mb - X_tilde)**2)  # MSE loss\n",
    "\n",
    "            E_optimizer.zero_grad()\n",
    "            E_loss.backward()\n",
    "            E_optimizer.step()\n",
    "            wandb.log({\"Embedding Loss\": E_loss})\n",
    "            \n",
    "    # Supervised Training Phase\n",
    "    # Here, the intention was to train the generator with supervised loss,\n",
    "    # but it mistakenly updates using the embedder optimizer.\n",
    "    # Adjusting to reflect the correct training intention.\n",
    "    for epoch in range(num_epochs_supervised):\n",
    "        for X_mb, T_mb in data_loader:\n",
    "            Z_mb = torch.randn([batch_size, seq_len, hidden_dim])\n",
    "            H_mb = embedder(X_mb)\n",
    "            E_hat = generator(Z_mb)\n",
    "            H_hat_supervised = supervisor(E_hat)  # Generate supervised embeddings\n",
    "            S_loss = torch.mean((H_mb - H_hat_supervised)**2)  # Supervised loss for generator\n",
    "\n",
    "            G_optimizer.zero_grad()\n",
    "            S_loss.backward()\n",
    "            G_optimizer.step()\n",
    "            wandb.log({\"Supervised Loss\": S_loss})\n",
    "            \n",
    "    # Adversarial Training Phase\n",
    "    for epoch in range(num_epochs_adversarial):\n",
    "        for X_mb, T_mb in data_loader:\n",
    "            # Discriminator Training\n",
    "            for _ in range(2):  # Typically, discriminator is trained more frequently.\n",
    "                Z_mb = torch.randn([batch_size, seq_len, hidden_dim])\n",
    "                H_mb = embedder(X_mb)\n",
    "                H_fake = generator(Z_mb).detach()  # Detach to avoid training generator here.\n",
    "\n",
    "                D_real = discriminator(H_mb)\n",
    "                D_fake = discriminator(H_fake)\n",
    "\n",
    "                D_loss = -torch.mean(torch.log(D_real + 1e-8) + torch.log(1 - D_fake + 1e-8))\n",
    "\n",
    "                D_optimizer.zero_grad()\n",
    "                D_loss.backward()\n",
    "                D_optimizer.step()\n",
    "                wandb.log({\"Discriminator Loss\": D_loss})\n",
    "                \n",
    "            # Generator Training\n",
    "            Z_mb = torch.randn([batch_size, seq_len, hidden_dim])\n",
    "            H_fake = generator(Z_mb)\n",
    "\n",
    "            D_fake = discriminator(H_fake)\n",
    "            G_loss = -torch.mean(torch.log(D_fake + 1e-8))\n",
    "\n",
    "            G_optimizer.zero_grad()\n",
    "            G_loss.backward()\n",
    "            G_optimizer.step()\n",
    "            wandb.log({\"Generator Loss\": G_loss})\n",
    "            \n",
    "    return embedder, recovery, generator, supervisor, discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "def generate_synthetic_time_series(num_samples, seq_length, freq=1, noise_level=0.1, trend=False):\n",
    "    \"\"\"\n",
    "    Generates synthetic time series data.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_samples: Number of time series samples to generate.\n",
    "    - seq_length: Length of each time series.\n",
    "    - freq: Frequency of the sine wave.\n",
    "    - noise_level: Standard deviation of Gaussian noise added to the data.\n",
    "    - trend: If True, adds a linear trend to the data.\n",
    "    \n",
    "    Returns:\n",
    "    - data: Generated synthetic time series data of shape (num_samples, seq_length).\n",
    "    \"\"\"\n",
    "    time = np.linspace(0, 2 * np.pi, seq_length)\n",
    "    sine_wave = np.sin(freq * time)\n",
    "    \n",
    "    if trend:\n",
    "        trend = np.linspace(0, 1, seq_length)\n",
    "        sine_wave += trend\n",
    "    \n",
    "    data = np.zeros((num_samples, seq_length))\n",
    "    for i in range(num_samples):\n",
    "        noise = np.random.normal(0, noise_level, seq_length)\n",
    "        data[i, :] = sine_wave + noise\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_dataloader(data, batch_size=64):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader from the given time series data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Time series data of shape (num_samples, seq_length).\n",
    "    - batch_size: Size of each batch.\n",
    "    \n",
    "    Returns:\n",
    "    - DataLoader object.\n",
    "    \"\"\"\n",
    "    # Convert numpy array to PyTorch tensor\n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "    \n",
    "    # Create TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, data_tensor)  # Targets are same as inputs for generative models\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# Generate synthetic time series data\n",
    "num_samples = 1000  # Total number of time series samples\n",
    "seq_length = 100  # Length of each time series\n",
    "batch_size = 64  # Batch size for training\n",
    "data = generate_synthetic_time_series(num_samples, seq_length, freq=2, noise_level=0.2, trend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 PyTorch 텐서로 변환하고, 마지막 차원을 추가합니다.\n",
    "data_tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# TensorDataset과 DataLoader 생성\n",
    "dataset = TensorDataset(data_tensor, data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 설정\n",
    "input_dim = 1\n",
    "hidden_dim = 24\n",
    "output_dim = 1\n",
    "num_layers = 3\n",
    "\n",
    "\n",
    "# 모델 인스턴스화\n",
    "embedder = Embedder(input_dim, hidden_dim, num_layers)\n",
    "recovery = Recovery(hidden_dim, output_dim, num_layers)\n",
    "generator = Generator(hidden_dim, hidden_dim, num_layers)\n",
    "supervisor = Supervisor(hidden_dim, hidden_dim, num_layers-1)\n",
    "discriminator = Discriminator(hidden_dim, hidden_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedder(\n",
       "   (rnn): GRU(1, 24, num_layers=3, batch_first=True)\n",
       "   (fc): Linear(in_features=24, out_features=24, bias=True)\n",
       " ),\n",
       " Recovery(\n",
       "   (rnn): GRU(24, 24, num_layers=3, batch_first=True)\n",
       "   (fc): Linear(in_features=24, out_features=1, bias=True)\n",
       " ),\n",
       " Generator(\n",
       "   (rnn): GRU(24, 24, num_layers=3, batch_first=True)\n",
       "   (fc): Linear(in_features=24, out_features=24, bias=True)\n",
       " ),\n",
       " Supervisor(\n",
       "   (rnn): GRU(24, 24, batch_first=True)\n",
       "   (fc): Linear(in_features=24, out_features=24, bias=True)\n",
       " ),\n",
       " Discriminator(\n",
       "   (rnn): GRU(24, 24, num_layers=3, batch_first=True, bidirectional=True)\n",
       "   (fc): Linear(in_features=48, out_features=1, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 옵티마이저\n",
    "E_optimizer = optim.Adam(list(embedder.parameters()) + list(recovery.parameters()), lr=0.001)\n",
    "G_optimizer = optim.Adam(list(generator.parameters()) + list(supervisor.parameters()), lr=0.001)\n",
    "D_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "# 훈련\n",
    "num_epochs_embedding = 10\n",
    "num_epochs_adversarial = 10\n",
    "num_epochs_supervised = 10\n",
    "train_GAN(num_epochs_embedding, num_epochs_adversarial, num_epochs_supervised, dataloader, batch_size, seq_length, hidden_dim, embedder, recovery, generator, supervisor, discriminator, E_optimizer, G_optimizer, D_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = []\n",
    "for _ in range(10):\n",
    "    Z_mb = torch.randn([batch_size, seq_length, hidden_dim])\n",
    "    H_fake = generator(Z_mb)\n",
    "    generated_samples.append(H_fake.detach().numpy())\n",
    "    \n",
    "generated_samples = np.array(generated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e529fe77a04ce4bdd3b012cc979874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.027 MB uploaded\\r'), FloatProgress(value=0.04887947767778366, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Discriminator Loss</td><td>▃▄▃▂▁▁█▇▄▃▃▂▂▁▁▁▁▂▆▆▅▄▃▃▃▃▃▃▃▃▂▂▁▁▁▁▆▂▂▇</td></tr><tr><td>Embedding Loss</td><td>████████▇▇▇▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Generator Loss</td><td>▂▂▂▄▇█▆▅▃▂▂▃▄▅▆▅▆▅▁▁▁▂▂▂▂▂▂▂▂▂▃▄▅▅▆▆▇▆▇▇</td></tr><tr><td>Supervised Loss</td><td>█▇▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Discriminator Loss</td><td>2.67575</td></tr><tr><td>Embedding Loss</td><td>0.12419</td></tr><tr><td>Generator Loss</td><td>2.7333</td></tr><tr><td>Supervised Loss</td><td>0.00139</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-shadow-18</strong> at: <a href='https://wandb.ai/se99an/Time-GAN/runs/6pqto32j' target=\"_blank\">https://wandb.ai/se99an/Time-GAN/runs/6pqto32j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240324_211258-6pqto32j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model params 저장\n",
    "wandb.watch(embedder)\n",
    "wandb.watch(recovery)\n",
    "wandb.watch(generator)\n",
    "wandb.watch(supervisor)\n",
    "wandb.watch(discriminator)\n",
    "\n",
    "wandb.save(\"model.h5\")\n",
    "wandb.save(\"model.pt\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_samples 저장\n",
    "import os\n",
    "os.makedirs(\"generated_samples\", exist_ok=True)\n",
    "np.save(\"generated_samples/generated_samples.npy\", generated_samples)\n",
    "\n",
    "\n",
    "\n",
    "# 모델 저장\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(embedder.state_dict(), \"models/embedder.pt\")\n",
    "torch.save(recovery.state_dict(), \"models/recovery.pt\")\n",
    "torch.save(generator.state_dict(), \"models/generator.pt\")\n",
    "torch.save(supervisor.state_dict(), \"models/supervisor.pt\")\n",
    "torch.save(discriminator.state_dict(), \"models/discriminator.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "embedder = Embedder(input_dim, hidden_dim, num_layers)\n",
    "recovery = Recovery(hidden_dim, output_dim, num_layers)\n",
    "generator = Generator(hidden_dim, hidden_dim, num_layers)\n",
    "supervisor = Supervisor(hidden_dim, hidden_dim, num_layers-1)\n",
    "discriminator = Discriminator(hidden_dim, hidden_dim, num_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
