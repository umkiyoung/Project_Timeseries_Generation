{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from timeganutils import random_generator, extract_time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Time_GAN_module(nn.Module):\n",
    "    \"\"\"\n",
    "    Class from which a module of the Time GAN Architecture can be constructed, \n",
    "    consisting of a n_layer stacked RNN layers and a fully connected layer\n",
    "    \n",
    "    input_size = dim of data (depending if module operates on latent or non-latent space)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, activation=torch.sigmoid, rnn_type=\"gru\"):\n",
    "        super(Time_GAN_module, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sigma = activation\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        if self.rnn_type == \"gru\":\n",
    "          self.rnn = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        elif self.rnn_type == \"rnn\":\n",
    "          self.rnn = nn.RNN(input_size, hidden_dim, num_layers, batch_first = True) \n",
    "        elif self.rnn_type == \"lstm\": # input params still the same for lstm\n",
    "          self.rnn = nn.LSTM(input_size, hidden_dim, num_layers, batch_first = True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            # Initializing hidden state for first input using method defined below\n",
    "            if self.rnn_type in [\"rnn\", \"gru\"]:\n",
    "              hidden = self.init_hidden(batch_size)\n",
    "            elif self.rnn_type == \"lstm\": # additional initial cell state for lstm\n",
    "              h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device).float()\n",
    "              c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device).float()\n",
    "              hidden = (h0, c0)\n",
    "            # Passing in the input and hidden state into the model and obtaining outputs\n",
    "            out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "            # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "            out = out.contiguous().view(-1, self.hidden_dim)\n",
    "            out = self.fc(out)\n",
    "            \n",
    "            if self.sigma == nn.Identity:\n",
    "                idendity = nn.Identity()\n",
    "                return idendity(out)\n",
    "                \n",
    "            out = self.sigma(out)\n",
    "            \n",
    "            # HIDDEN STATES WERDEN IN DER PAPER IMPLEMENTIERUNG AUCH COMPUTED, ALLERDINGS NICHT BENUTZT?\n",
    "            \n",
    "            return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden\n",
    "    \n",
    "def TimeGAN(data, parameters):\n",
    "  \"\"\"\n",
    "  Main workhorse function of timegan\n",
    "  Args: \n",
    "    - data = Time series data\n",
    "    - parameters = dictionary of training parameters\n",
    "  \n",
    "  hidden_dim = dimension of hidden layers, integer\n",
    "  num_layers = number of recurrent layers, integer\n",
    "  iterations = number of training iterations every epoch, integer  \n",
    "  batch_size = number of samples passed in every training batch, integer [32 - 128 works best]\n",
    "  epoch = number of training epochs\n",
    "  \"\"\"\n",
    "  hidden_dim = parameters[\"hidden_dim\"]\n",
    "  num_layers = parameters[\"num_layers\"]\n",
    "  iterations = parameters[\"iterations\"]\n",
    "  batch_size = parameters[\"batch_size\"]\n",
    "  module = parameters[\"module\"]\n",
    "  epoch = parameters[\"epoch\"]\n",
    "  no, seq_len, dim = np.asarray(data).shape\n",
    "  z_dim = dim\n",
    "  gamma = 1\n",
    "\n",
    "  checkpoints = {}\n",
    "\n",
    "  # instantiating every module we're going to train\n",
    "  Embedder = Time_GAN_module(input_size=z_dim, output_size=hidden_dim, hidden_dim=hidden_dim, n_layers=num_layers)\n",
    "  Recovery = Time_GAN_module(input_size=hidden_dim, output_size=dim, hidden_dim=hidden_dim, n_layers=num_layers)\n",
    "  Generator = Time_GAN_module(input_size=dim, output_size=hidden_dim, hidden_dim=hidden_dim, n_layers=num_layers)\n",
    "  Supervisor = Time_GAN_module(input_size=hidden_dim, output_size=hidden_dim, hidden_dim=hidden_dim, n_layers=num_layers)\n",
    "  Discriminator = Time_GAN_module(input_size=hidden_dim, output_size=1, hidden_dim=hidden_dim, n_layers=num_layers, activation=nn.Identity)\n",
    "\n",
    "  # instantiating all optimizers, \n",
    "  # learning rates chosen through experimentation and comparison with results in the paper\n",
    "  embedder_optimizer = optim.Adam(Embedder.parameters(), lr=0.0035)\n",
    "  recovery_optimizer = optim.Adam(Recovery.parameters(), lr=0.01)\n",
    "  supervisor_optimizer = optim.Adam(Recovery.parameters(), lr=0.001)\n",
    "  discriminator_optimizer = optim.Adam(Discriminator.parameters(), lr=0.01)\n",
    "  generator_optimizer = optim.Adam(Generator.parameters(), lr=0.01)\n",
    "  \n",
    "  # instantiating mse loss & Data Loader\n",
    "  binary_cross_entropy_loss = nn.BCEWithLogitsLoss()\n",
    "  MSE_loss = nn.MSELoss()\n",
    "  loader = DataLoader(data, parameters['batch_size'], shuffle=False)\n",
    "  random_data = random_generator(batch_size=parameters['batch_size'], z_dim=dim, \n",
    "                                       T_mb=extract_time(data)[0], max_seq_len=extract_time(data)[1])\n",
    "  \n",
    "  # Embedding Network Training\n",
    "  # Here we train embedding & Recovery network jointly\n",
    "  print('Start Embedding Network Training')\n",
    "  for e in range(epoch): \n",
    "    for batch_index, X in enumerate(loader):\n",
    "                \n",
    "        H, _ = Embedder(X.float())\n",
    "        H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "        X_tilde, _ = Recovery(H)\n",
    "        X_tilde = torch.reshape(X_tilde, (batch_size, seq_len, dim))\n",
    "\n",
    "        # constants chosen like in the paper\n",
    "        E_loss0 = 10 * torch.sqrt(MSE_loss(X, X_tilde))  \n",
    "\n",
    "        Embedder.zero_grad()\n",
    "        Recovery.zero_grad()\n",
    "\n",
    "        E_loss0.backward(retain_graph=True)\n",
    "\n",
    "        embedder_optimizer.step()\n",
    "        recovery_optimizer.step()\n",
    "\n",
    "        if e in range(1,epoch) and batch_index == 0:\n",
    "            print('step: '+ str(e) + '/' + str(epoch) + ', e_loss: ' + str(np.sqrt(E_loss0.detach().numpy())))\n",
    "\n",
    "  print('Finish Embedding Network Training')\n",
    "\n",
    "  # Training only with supervised loss\n",
    "  # here the embedder and supervisor are trained jointly\n",
    "  print('Start Training with Supervised Loss Only')\n",
    "  for e in range(epoch): \n",
    "    for batch_index, X in enumerate(loader):\n",
    "\n",
    "        H, _ = Embedder(X.float())\n",
    "        H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "        H_hat_supervise, _ = Supervisor(H)\n",
    "        H_hat_supervise = torch.reshape(H_hat_supervise, (batch_size, seq_len, hidden_dim))  \n",
    "\n",
    "        G_loss_S = MSE_loss(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "\n",
    "\n",
    "        Embedder.zero_grad()\n",
    "        Supervisor.zero_grad()\n",
    "\n",
    "        G_loss_S.backward(retain_graph=True)\n",
    "\n",
    "        embedder_optimizer.step()\n",
    "        supervisor_optimizer.step()\n",
    "\n",
    "        if e in range(1,epoch) and batch_index == 0:\n",
    "            print('step: '+ str(e) + '/' + str(epoch) + ', s_loss: ' + str(np.sqrt(G_loss_S.detach().numpy())))\n",
    "\n",
    "  print('Finish Training with Supervised Loss Only')\n",
    "  \n",
    "  \n",
    "  # Joint Training\n",
    "  print('Start Joint Training')\n",
    "  for itt in range(epoch):\n",
    "    # the generator, supervisor and discriminator are trained for an extra two steps\n",
    "    # in the issues on github the author mentions, the marked constant of 0.15 (below)\n",
    "    # had been chosen because it worked well in experiments, and to keep the balance in\n",
    "    # training the generator and discriminator.\n",
    "    for kk in range(2):\n",
    "      X = next(iter(loader))\n",
    "      random_data #= random_generator(batch_size=batch_size, z_dim=dim, \n",
    "                                       #T_mb=extract_time(data)[0], max_seq_len=extract_time(data)[1])\n",
    "        \n",
    "      # Generator Training \n",
    "      ## Train Generator\n",
    "      z = torch.tensor(random_data)\n",
    "      z = z.float()\n",
    "        \n",
    "      e_hat, _ = Generator(z)\n",
    "      e_hat = torch.reshape(e_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "      H_hat, _ = Supervisor(e_hat)\n",
    "      H_hat = torch.reshape(H_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "      Y_fake = Discriminator(H_hat)\n",
    "      Y_fake = torch.reshape(Y_fake, (batch_size, seq_len, 1))\n",
    "        \n",
    "      x_hat, _ = Recovery(H_hat)\n",
    "      x_hat = torch.reshape(x_hat, (batch_size, seq_len, dim))\n",
    "        \n",
    "      H, _ = Embedder(X.float())\n",
    "      H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      H_hat_supervise, _ = Supervisor(H)\n",
    "      H_hat_supervise = torch.reshape(H_hat_supervise, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      Generator.zero_grad()\n",
    "      Supervisor.zero_grad()\n",
    "      Discriminator.zero_grad()\n",
    "      Recovery.zero_grad()\n",
    "\n",
    "      # line 267 of original implementation: \n",
    "      # G_loss_U, G_loss_S, G_loss_V\n",
    "      G_loss_S = MSE_loss(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "      binary_cross_entropy_loss = nn.BCEWithLogitsLoss()\n",
    "      # logits first, then targets\n",
    "      # D_loss_real(Y_real, torch.ones_like(Y_real))\n",
    "      G_loss_U = binary_cross_entropy_loss(Y_fake, torch.ones_like(Y_fake))\n",
    "        \n",
    "      G_loss_V1 = torch.mean(torch.abs((torch.std(x_hat, [0], unbiased = False)) + 1e-6 - (torch.std(X, [0]) + 1e-6)))\n",
    "      G_loss_V2 = torch.mean(torch.abs((torch.mean(x_hat, [0]) - (torch.mean(X, [0])))))\n",
    "      G_loss_V = G_loss_V1 + G_loss_V2\n",
    "        \n",
    "      # doing a backward step for each loss should result in gradients accumulating \n",
    "      # so we should be able to optimize them jointly\n",
    "      G_loss_S.backward(retain_graph=True)#\n",
    "      G_loss_U.backward(retain_graph=True)\n",
    "      G_loss_V.backward(retain_graph=True)#\n",
    "\n",
    "\n",
    "      generator_optimizer.step()\n",
    "      supervisor_optimizer.step()\n",
    "      discriminator_optimizer.step()\n",
    "      # Train Embedder \n",
    "      ## line 270: we only optimize E_loss_T0\n",
    "      ## E_loss_T0 = just mse of x and x_tilde\n",
    "      # but it calls E_solver which optimizes E_loss, which is a sum of \n",
    "      # E_loss0 and 0.1* G_loss_S\n",
    "      MSE_loss = nn.MSELoss()\n",
    "        \n",
    "      H, _ = Embedder(X.float())\n",
    "      H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      X_tilde, _ = Recovery(H)\n",
    "      X_tilde = torch.reshape(X_tilde, (batch_size, seq_len, dim))\n",
    "\n",
    "      E_loss_T0 = MSE_loss(X, X_tilde)\n",
    "      E_loss0 = 10 * torch.sqrt(MSE_loss(X, X_tilde))  \n",
    "        \n",
    "      H_hat_supervise, _ = Supervisor(H)\n",
    "      H_hat_supervise = torch.reshape(H_hat_supervise, (batch_size, seq_len, hidden_dim))  \n",
    "\n",
    "      G_loss_S = MSE_loss(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "      E_loss = E_loss0  + 0.1 * G_loss_S\n",
    "        \n",
    "      G_loss_S.backward(retain_graph=True)\n",
    "      E_loss_T0.backward()\n",
    "        \n",
    "      Embedder.zero_grad()\n",
    "      Recovery.zero_grad()\n",
    "      Supervisor.zero_grad()\n",
    "        \n",
    "      embedder_optimizer.step()\n",
    "      recovery_optimizer.step()\n",
    "      supervisor_optimizer.step()\n",
    "    # train Discriminator\n",
    "    for batch_index, X in enumerate(loader):\n",
    "      random_data #= random_generator(batch_size=batch_size, z_dim=dim, \n",
    "                                       #T_mb=extract_time(data)[0], max_seq_len=extract_time(data)[1])\n",
    "      \n",
    "      z = torch.tensor(random_data)\n",
    "      z = z.float()\n",
    "\n",
    "      H, _ = Embedder(X)\n",
    "      H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      Y_real = Discriminator(H)\n",
    "      Y_real = torch.reshape(Y_real, (batch_size, seq_len, 1))\n",
    "      \n",
    "      e_hat, _ = Generator(z)\n",
    "      e_hat = torch.reshape(e_hat, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      Y_fake_e = Discriminator(e_hat)\n",
    "      Y_fake_e = torch.reshape(Y_fake_e, (batch_size, seq_len, 1))\n",
    "        \n",
    "      H_hat, _ = Supervisor(e_hat)\n",
    "      H_hat = torch.reshape(H_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "      Y_fake = Discriminator(H_hat)\n",
    "      Y_fake = torch.reshape(Y_fake, (batch_size, seq_len, 1))\n",
    "        \n",
    "      x_hat, _ = Recovery(H_hat)\n",
    "      x_hat = torch.reshape(x_hat, (batch_size, seq_len, dim))\n",
    "\n",
    "      Generator.zero_grad()\n",
    "      Supervisor.zero_grad()\n",
    "      Discriminator.zero_grad()\n",
    "      Recovery.zero_grad()\n",
    "      Embedder.zero_grad()\n",
    "\n",
    "      # logits first, then targets\n",
    "      # D_loss_real(Y_real, torch.ones_like(Y_real))\n",
    "      D_loss_real = nn.BCEWithLogitsLoss()\n",
    "      DLR = D_loss_real(Y_real, torch.ones_like(Y_real))\n",
    "\n",
    "      D_loss_fake = nn.BCEWithLogitsLoss()\n",
    "      DLF = D_loss_fake(Y_fake, torch.zeros_like(Y_fake))\n",
    "\n",
    "      D_loss_fake_e = nn.BCEWithLogitsLoss()\n",
    "      DLF_e = D_loss_fake_e(Y_fake_e, torch.zeros_like(Y_fake_e))\n",
    "\n",
    "      D_loss = DLR + DLF + gamma * DLF_e\n",
    "\n",
    "      # check discriminator loss before updating\n",
    "      check_d_loss = D_loss\n",
    "      # This is the magic number 0.15 we mentioned above. Set exactly like in the original implementation\n",
    "      if (check_d_loss > 0.15):\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        discriminator_optimizer.step()        \n",
    "        \n",
    "      H, _ = Embedder(X.float())\n",
    "      H = torch.reshape(H, (batch_size, seq_len, hidden_dim)) \n",
    "        \n",
    "      X_tilde, _ = Recovery(H)\n",
    "      X_tilde = torch.reshape(X_tilde, (batch_size, seq_len, dim))\n",
    "\n",
    "      \n",
    "      z = torch.tensor(random_data)\n",
    "      z = z.float()\n",
    "        \n",
    "      e_hat, _ = Generator(z)\n",
    "      e_hat = torch.reshape(e_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "      H_hat, _ = Supervisor(e_hat)\n",
    "      H_hat = torch.reshape(H_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "      Y_fake = Discriminator(H_hat)\n",
    "      Y_fake = torch.reshape(Y_fake, (batch_size, seq_len, 1))\n",
    "        \n",
    "      x_hat, _ = Recovery(H_hat)\n",
    "      x_hat = torch.reshape(x_hat, (batch_size, seq_len, dim))\n",
    "        \n",
    "      H, _ = Embedder(X.float())\n",
    "      H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      H_hat_supervise, _ = Supervisor(H)\n",
    "      H_hat_supervise = torch.reshape(H_hat_supervise, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      G_loss_S = MSE_loss(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "      binary_cross_entropy_loss = nn.BCEWithLogitsLoss()\n",
    "      # logits first then targets\n",
    "      G_loss_U = binary_cross_entropy_loss(Y_fake, torch.ones_like(Y_fake))\n",
    "        \n",
    "      G_loss_V1 = torch.mean(torch.abs((torch.std(x_hat, [0], unbiased = False)) + 1e-6 - (torch.std(X, [0]) + 1e-6)))\n",
    "      G_loss_V2 = torch.mean(torch.abs((torch.mean(x_hat, [0]) - (torch.mean(X, [0])))))\n",
    "      G_loss_V = G_loss_V1 + G_loss_V2\n",
    "    \n",
    "      E_loss_T0 = MSE_loss(X, X_tilde)\n",
    "      E_loss0 = 10 * torch.sqrt(MSE_loss(X, X_tilde))  \n",
    "      E_loss = E_loss0  + 0.1 * G_loss_S\n",
    "        \n",
    "      # doing a backward step for each loss should result in gradients accumulating \n",
    "      # so we should be able to optimize them jointly\n",
    "      G_loss_S.backward(retain_graph=True)#\n",
    "      G_loss_U.backward(retain_graph=True)\n",
    "      G_loss_V.backward(retain_graph=True)#\n",
    "      E_loss.backward()\n",
    "\n",
    "      generator_optimizer.step()\n",
    "      supervisor_optimizer.step()\n",
    "      embedder_optimizer.step()\n",
    "      recovery_optimizer.step()\n",
    "            \n",
    "      print('step: '+ str(itt) + '/' + str(epoch) + \n",
    "            ', D_loss: ' + str(D_loss.detach().numpy()) +\n",
    "            ', G_loss_U: ' + str(G_loss_U.detach().numpy()) + \n",
    "            ', G_loss_S: ' + str(G_loss_S.detach().numpy()) + \n",
    "            ', E_loss_t0: ' + str(np.sqrt(E_loss0.detach().numpy())))\n",
    "         \n",
    "\n",
    "      \n",
    "      random_test = random_generator(1, dim, extract_time(data)[0], extract_time(data)[1])        \n",
    "      test_sample = Generator(torch.tensor(random_generator(1, dim, extract_time(data)[0], extract_time(data)[1])).float())[0]      \n",
    "      test_sample = torch.reshape(test_sample, (1, seq_len, hidden_dim))\n",
    "      test_recovery = Recovery(test_sample)\n",
    "      test_recovery = torch.reshape(test_recovery[0], (1, seq_len, dim))\n",
    "      fig, ax = plt.subplots()\n",
    "      ax1 = plt.plot(test_recovery[0].detach().numpy())\n",
    "      plt.show()\n",
    "      \n",
    "      if itt % 2:\n",
    "        checkpoints[itt] = [Generator.state_dict(), Discriminator.state_dict(), Embedder.state_dict(), Recovery.state_dict,\n",
    "                    Supervisor.state_dict()]\n",
    "             \n",
    "  print('Finish Joint Training')\n",
    "                \n",
    "  return Generator, Embedder, Supervisor, Recovery, Discriminator, checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
